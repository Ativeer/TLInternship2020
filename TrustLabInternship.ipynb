{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Negative Sentiments towards HydroxyChloroquine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Important Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comcrawl import IndexClient\n",
    "import pandas as pd\n",
    "from urllib.request import Request, urlopen\n",
    "import nltk\n",
    "# nltk.download('stopwords # download only if not present\n",
    "import bs4 as bs\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn import model_selection, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler class uses the comcrawl API\n",
    "This finds all the links found within a specif domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    def __init__(self, key_points):\n",
    "        self.key_points = key_points\n",
    "        print(\"Crawler Imported\")\n",
    "    \n",
    "    def client_search(self, threads=4):\n",
    "        print(\"Initiating Client search\")\n",
    "        client = IndexClient(self.key_points['key_index'])\n",
    "        client.search(self.key_points[\"website\"], threads=6)\n",
    "        client.download()\n",
    "        return client\n",
    "    \n",
    "    def client_to_df(self, client):\n",
    "        print(\"Exporting Dataset/Downloaded file\")\n",
    "        dataset = (pd.DataFrame(client.results).sort_values(by=\"timestamp\").drop_duplicates(\"urlkey\", keep=\"last\").to_dict(\"records\"))        \n",
    "        pd.DataFrame(client.results).to_csv(self.key_points[\"dataframe_path\"])\n",
    "#         dataframe_path = \"output.csv\"\n",
    "    \n",
    "    def read_df(self):\n",
    "        print(\"Reading File as a DataFrame\")\n",
    "        return pd.read_csv(self.key_points[\"dataframe_path\"])\n",
    "    \n",
    "    def fetch_urls(self, database):\n",
    "        self.key_points[\"url_with_key_word\"] = database[database[\"url\"].str.contains(self.key_points[\"search_word\"])]\n",
    "        database[\"html\"] = database[\"html\"].fillna(\" \")\n",
    "        self.key_points[\"html_with_key_word\"] = database[database[\"html\"].str.contains(self.key_points[\"search_word\"])]\n",
    "    \n",
    "    def main(self):\n",
    "        print(\"Crawler Begins\")\n",
    "        client = self.client_search(self.key_points[\"threads\"])\n",
    "        self.client_to_df(client)\n",
    "        database = self.read_df()\n",
    "        \n",
    "        print(\"Crawler Work Completed!\")\n",
    "        \n",
    "        return database\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WebPageReader Class using BeautifulSoup\n",
    "Reading paragraphs and title within the link\n",
    "\n",
    "Also, used for cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebPageReader:\n",
    "    def __init__(self, key_points):\n",
    "        self.key_points = key_points\n",
    "        print(\"Web Page Reader Imported\")\n",
    "\n",
    "    def webpage_url(self, url):\n",
    "        try:\n",
    "            req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            webpage = urlopen(req).read()\n",
    "            soup = bs.BeautifulSoup(webpage,'lxml')\n",
    "            return soup.title.string, soup\n",
    "        except:\n",
    "            return \"Page Not valid\", 0\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        print(\"Cleaning Text\")\n",
    "        x= re.sub(r'&[@#$%&()0-9]*',r'',text)\n",
    "        x = re.sub(r'https?\\S+',r'',x)\n",
    "        x = re.sub('<[^>]*>', '', x)\n",
    "\n",
    "        x = x.translate(str.maketrans(string.punctuation,' '*len(string.punctuation)))\n",
    "        tokens = x.split()\n",
    "        ps = PorterStemmer()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [ps.stem(token) for token in tokens if token not in stop_words] \n",
    "        return ' '.join([token for token in tokens if token not in stop_words])\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FindHateSpeech Class\n",
    "Using NLTKs sentimentintensityanalyzer function to find the polarity score of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindHateSpeech:\n",
    "    def __init__(self, key_points):\n",
    "        self.key_points = key_points\n",
    "        print(\"Find Hate Speech Imported\")\n",
    "        self.sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def polarity_score_finder(self, text):\n",
    "        score = self.sid.polarity_scores(str(text.text))\n",
    "        if score['compound'] > 0 and score['pos'] > self.key_points[\"positive_threshold\"]:\n",
    "            return self.sid.polarity_scores(str(text.text))\n",
    "        else:\n",
    "            return \" \"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Class\n",
    "Combining all classes to perform sentiment analysis on text and storing the links which provide a positive impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLinternship:\n",
    "    def __init__(self, key_points):\n",
    "        self.key_points = key_points\n",
    "        print(\"Initiating Code!\")\n",
    "        self.check_text = {}\n",
    "        self.output = []\n",
    "    \n",
    "    def parse_urls(self, web, sentiment):\n",
    "        print(f\"Total URL Containing {self.key_points['search_word']}: {len(self.key_points['url_with_key_word'])}\")\n",
    "\n",
    "        for links in self.key_points['url_with_key_word'][\"url\"]:\n",
    "            title, _ = web.webpage_url(links)\n",
    "            if title == \"Page Not valid\":\n",
    "                continue\n",
    "            else:\n",
    "                self.check_text[links] = title\n",
    "    \n",
    "    def parse_html(self, web, sentiment):\n",
    "        print(f\"Total HTML Containing {self.key_points['search_word']}: {len(self.key_points['html_with_key_word'])}\")        \n",
    "        soup = []\n",
    "        \n",
    "        for x in self.key_points['html_with_key_word'][\"url\"]:\n",
    "            title, s = web.webpage_url(x)\n",
    "            if title == \"Page Not valid\":\n",
    "                continue\n",
    "            else:\n",
    "                for paragraph in s.find_all('p'):\n",
    "                    if self.key_points[\"search_word\"] in str(paragraph.text):\n",
    "                        self.check_text[x] = paragraph        \n",
    "    \n",
    "    def main(self):\n",
    "        print(\"Inititiating Crawler\")\n",
    "        crawl = Crawler(self.key_points)\n",
    "        database = crawl.main()\n",
    "\n",
    "#         Run the below two lines and comment out the above line, if you have the 'ouptut.csv' for faster run\n",
    "#         database = pd.read_csv('output.csv')\n",
    "#         crawl.fetch_urls(database)\n",
    "        \n",
    "        web = WebPageReader(self.key_points)\n",
    "        sentiment = FindHateSpeech(self.key_points)\n",
    "        \n",
    "        self.parse_urls(web, sentiment)\n",
    "        self.parse_html(web, sentiment)\n",
    "        for key, values in self.check_text.items():\n",
    "            score = sentiment.polarity_score_finder(values)\n",
    "            if score == ' ':\n",
    "                continue\n",
    "            else:\n",
    "                self.output.append(key)\n",
    "        \n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_points = {\n",
    "    \"key_index\" : [\"2020-29\", \"2020-24\",\"2020-16\",\"2020-10\", \"2020-05\"],\n",
    "    \"website\" : \"fda.gov/drugs/*\",\n",
    "    \"dataframe_path\" : \"output.csv\",\n",
    "    \"search_word\" : \"hydroxychloroquine\",\n",
    "    \"positive_threshold\" : 0.02,\n",
    "    \"threads\" : 6\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating Code!\n"
     ]
    }
   ],
   "source": [
    "tli = TLinternship(key_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inititiating Crawler\n",
      "Crawler Imported\n",
      "Web Page Reader Imported\n",
      "Find Hate Speech Imported\n",
      "Total URL Containing hydroxychloroquine: 5\n",
      "Total HTML Containing hydroxychloroquine: 34\n"
     ]
    }
   ],
   "source": [
    "imp_links = tli.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.fda.gov/drugs/drug-safety-and-availability/fda-cautions-against-use-hydroxychloroquine-or-chloroquine-covid-19-outside-hospital-setting-or?fbclid=IwAR1-PZHsA-1A0tRm_ywuoe6c69_2--rqJoGVOX7wcdNRqSQBvi-rrajxF5o',\n",
       " 'https://www.fda.gov/drugs/drug-safety-and-availability/fda-cautions-against-use-hydroxychloroquine-or-chloroquine-covid-19-outside-hospital-setting-or?utm_campaign',\n",
       " 'https://www.fda.gov/drugs/drug-safety-and-availability/fda-cautions-against-use-hydroxychloroquine-or-chloroquine-covid-19-outside-hospital-setting-or?utm_source=042720-RC20%20COVID%20FDA%20Issues%20New%20Guidance&utm_medium=email&utm_campaign=RC20']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
